{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Lab WiSe 2024/2025: Baseline Retrieval System\n",
    "\n",
    "This Jupyter notebook serves as a baseline retrieval system that you can improve upon.\n",
    "We use subsets of the MS MARCO datasets to retrieve passages of web documents.\n",
    "We will show you how to create a software submission to TIRA from this notebook.\n",
    "\n",
    "An overview of all corpora that we use in the current course is available at [https://tira.io/datasets?query=ir-lab-wise-2024](https://tira.io/datasets?query=ir-lab-wise-2024). The dataset IDs for loading the datasets are:\n",
    "\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training`: A subsample of the TREC 2019/2020 Deep Learning tracks on the MS MARCO v1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-rag-20241202-training` (_work in progress_): A subsample of the TREC 2024 Retrieval-Augmented Generation track on the MS MARCO v2.1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/ms-marco-rag-20241203-test` (work in progress): The test corpus that we have created together in the course, based on the MS MARCO v2.1 passage dataset. We will use this dataset as the test dataset, i.e., evaluation scores become available only after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries\n",
    "\n",
    "We will use [tira](https://tira.io/), an information retrieval shared task platform, and [ir_dataset](https://ir-datasets.com/) for loading the datasets. Subsequently, we will build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install 'tira>=0.0.139' ir-datasets 'python-terrier==0.10.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://files.webis.de/software/pyterrier-plugins/custom-terrier-token-processing-1.0-SNAPSHOT-jar-with-dependencies.jar -O /root/.pyterrier/custom-terrier-token-processing-0.0.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1'])\n",
    "    from jnius import autoclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an API client to interact with the TIRA platform (e.g., to load datasets and submit runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset\n",
    "\n",
    "We load the dataset by its ir_datasets ID (as listed in the Readme). Just be sure to add the `irds:` prefix before the dataset ID to tell PyTerrier to load the data from ir_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import get_dataset\n",
    "\n",
    "pt_dataset = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build an index\n",
    "\n",
    "We will then create an index from the documents in the dataset we just loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Task is it to test different Stemmers/Lemmatizers and check which of them gives the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import IterDictIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_porter = IterDictIndexer(\n",
    "    # Store the index in the `index` directory.\n",
    "    \"../data/index\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    "    stemmer='PorterStemmer'\n",
    ")\n",
    "index_porter = indexer_porter.index(pt_dataset.get_corpus_iter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_none = IterDictIndexer(\n",
    "    \"../data/index_none\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    overwrite=True,\n",
    "    stemmer = None\n",
    ")\n",
    "\n",
    "index_none= indexer_none.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_Snowball = IterDictIndexer(\n",
    "    \"../data/index_Snowball\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    overwrite=True,\n",
    "    stemmer = 'EnglishSnowballStemmer'\n",
    ")\n",
    "\n",
    "index_Snowball = indexer_Snowball.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_LemurKrovetz = IterDictIndexer(\n",
    "    \"../data/index_LemurKrovetz\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    overwrite=True,\n",
    "    stemmer = 'LemurKrovetzStemmer'\n",
    ")\n",
    "\n",
    "index_LemurKrovetz = indexer_LemurKrovetz.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_standfordLemmatizer = IterDictIndexer(\n",
    "    \"../data/index_standfordLemmatizer\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    overwrite=True,\n",
    "    stemmer = 'StanfordLemmatizer'\n",
    ")\n",
    "\n",
    "index_standfordLemmatizer = indexer_standfordLemmatizer.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import BatchRetrieve\n",
    "\n",
    "bm25_porter = BatchRetrieve(index_porter, wmodel=\"BM25\")\n",
    "bm25_none = BatchRetrieve(index_none, wmodel=\"BM25\")\n",
    "bm25_snowball = BatchRetrieve(indexer_Snowball, wmodel=\"BM25\")\n",
    "bm25_lemurkrovetz = BatchRetrieve(indexer_LemurKrovetz, wmodel=\"BM25\")\n",
    "bm25_standfordLemmatizer = BatchRetrieve(index_standfordLemmatizer, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import Experiment \n",
    "\n",
    "Experiment(\n",
    "    [bm25_porter, bm25_none, bm25_snowball, bm25_lemurkrovetz, bm25_standfordLemmatizer],\n",
    "    topics = pt_dataset.get_topics(\"text\"),\n",
    "    qrels = pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut_10\"],\n",
    "    names=[\"BM25 with Porter Stemmer\", \"BM25 with no Stemmer\", \"BM25 with Snowball Stemmer\", \"BM25 with LemurKrovetz Stemmer\", \"BM25 with Standford Lemmatizer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interim Conclusion\n",
    "This shows that the LemurKrovetzStemmer gives the best results. So now we start tuning the hyperparameters for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ir_datasets, ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "import pyterrier as pt\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "validation_dataset = 'ir-lab-jena-leipzig-wise-2023/validation-20231104-training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(training_dataset)\n",
    "queries = pt.io.read_topics(ir_datasets.topics_file(training_dataset), format='trecxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Index with the LemurKrovetzStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(documents):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 50, 'text': 4096}, stemmer = 'LemurKrovetzStemmer')\n",
    "    index_ref = indexer.index(({'docno': i.doc_id, 'text': i.text} for i in documents))\n",
    "    return pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index(dataset.docs_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the retrieval pipeline\n",
    "\n",
    "We will define a simple retrieval pipeline using just BM25 as a baseline. For details, refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io) or [tutorial](https://github.com/terrier-org/ecir2021tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ir_datasets, ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "def run_bm25_grid_search_run(index, output_dir, queries):\n",
    "    \"\"\"\n",
    "        defaults: http://terrier.org/docs/current/javadoc/org/terrier/matching/models/BM25.html\n",
    "        k_1 = 1.2d, k_3 = 8d, b = 0.75d\n",
    "        We do not tune parameter k_3, as this parameter only impacts queries with reduntant terms.\n",
    "    \"\"\"\n",
    "    for b in [0.7, 0.75, 0.8]:\n",
    "        for k_1 in [1.1, 1.2, 1.3]:\n",
    "            system = f'bm25-b={b}-k_1={k_1}'\n",
    "            configuration = {\"bm25.b\" : b, \"bm25.k_1\": k_1}\n",
    "            run_output_dir = output_dir + '/' + system\n",
    "            !rm -Rf {run_output_dir}\n",
    "            !mkdir -p {run_output_dir}\n",
    "            print(f'Run {system}')\n",
    "            BM25 = pt.BatchRetrieve(index, wmodel=\"BM25\", controls=configuration, verbose=True)\n",
    "            run = BM25(queries)\n",
    "            persist_and_normalize_run(run, system, run_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gridsearch on the given data takes roughly two hours. Since we only have 9 combinations of the given parameters, we just tried everyone of them manually to evaluate the best combination faster. The code below can still be executed, but if you want to take the shortcut you can just skip to the manual testing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bm25_grid_search_run(index, 'grid-search/training', queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = ir_datasets.load(validation_dataset)\n",
    "queries_val = pt.io.read_topics(ir_datasets.topics_file(validation_dataset), format='trecxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index(dataset.dos_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bm25_grid_search_run(index, 'grid-search/validation', queries_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tira trectools python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval\n",
    "from tira.rest_api_client import Client\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "tira = Client()\n",
    "\n",
    "def load_qrels(dataset):\n",
    "    return TrecQrel(tira.download_dataset('ir-lab-jena-leipzig-wise-2023', dataset, truth_dataset=True) + '/qrels.txt')\n",
    "\n",
    "training_qrels = load_qrels('training-20231104-training')\n",
    "validation_qrels = load_qrels('validation-20231104-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_run(run_dir, qrels):\n",
    "    run = TrecRun(run_dir + '/run.txt')\n",
    "    trec_eval = TrecEval(run, qrels)\n",
    "\n",
    "    return {\n",
    "        'run': run.get_runid(),\n",
    "        'nDCG@10': trec_eval.get_ndcg(depth=10),\n",
    "        'nDCG@10 (unjudgedRemoved)': trec_eval.get_ndcg(depth=10, removeUnjudged=True),\n",
    "        'MAP': trec_eval.get_map(depth=10),\n",
    "        'MRR': trec_eval.get_reciprocal_rank()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for r in glob('grid-search/training/bm25*'):\n",
    "    df += [evaluate_run(r, training_qrels)]\n",
    "df = pd.DataFrame(df)\n",
    "df.sort_values('nDCG@10', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for r in glob('grid-search/validation/bm25*'):\n",
    "    df += [evaluate_run(r, validation_qrels)]\n",
    "df = pd.DataFrame(df)\n",
    "df.sort_values('nDCG@10', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5: Manual testing\n",
    "As explained above, we testet all possible combinations of the given hyperparameters to tune the pipeline manually and safe time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration1 = {\"bm25.b\": 0.7, \"bm25.k_1\": 1.1}\n",
    "configuration2 = {\"bm25.b\": 0.75, \"bm25.k_1\": 1.1}\n",
    "configuration3 = {\"bm25.b\": 0.8, \"bm25.k_1\": 1.1}\n",
    "configuration4 = {\"bm25.b\": 0.7, \"bm25.k_1\": 1.2}\n",
    "configuration5 = {\"bm25.b\": 0.75, \"bm25.k_1\": 1.2}\n",
    "configuration6 = {\"bm25.b\": 0.8, \"bm25.k_1\": 1.2}\n",
    "configuration7 = {\"bm25.b\": 0.7, \"bm25.k_1\": 1.3}\n",
    "configuration8 = {\"bm25.b\": 0.75, \"bm25.k_1\": 1.3}\n",
    "configuration9 = {\"bm25.b\": 0.8, \"bm25.k_1\": 1.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indexer_tunedParameters once\n",
    "indexer_tunedParameters = IterDictIndexer(\n",
    "    \"../data/index_tunedParameters\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    overwrite=True,\n",
    "    stemmer='LemurKrovetzStemmer',\n",
    ")\n",
    "index_tunedParameters = indexer_tunedParameters.index(pt_dataset.get_corpus_iter())\n",
    "\n",
    "bm25_1 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration1)\n",
    "bm25_2 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration2)\n",
    "bm25_3 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration3)\n",
    "bm25_4 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration4)\n",
    "bm25_5 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration5)\n",
    "bm25_6 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration6)\n",
    "bm25_7 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration7)\n",
    "bm25_8 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration8)\n",
    "bm25_9 = BatchRetrieve(index_tunedParameters, wmodel=\"BM25\", controls=configuration9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the comibnations and evaluate the best ndcg_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import Experiment\n",
    "\n",
    "Experiment(\n",
    "    [bm25_1, bm25_2, bm25_3, bm25_4, bm25_5, bm25_6, bm25_7, bm25_8, bm25_9],\n",
    "    topics = pt_dataset.get_topics(\"text\"),\n",
    "    qrels = pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut_10\"],\n",
    "    names=[\"BM25 with tuned parameters 1\", \"BM25 with tuned parameters 2\", \"BM25 with tuned parameters 3\", \"BM25 with tuned parameters 4\", \"BM25 with tuned parameters 5\", \"BM25 with tuned parameters 6\", \"BM25 with tuned parameters 7\", \"BM25 with tuned parameters 8\", \"BM25 with tuned parameters 9\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that combination 4 (b = 0.7, k_1 = 1.2) has the best results (improvement  ~0,001 compared to the variant that is not tuned at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the run\n",
    "In the next steps, we would like to apply our retrieval system to some topics, to prepare a 'run' file, containing the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a short look at the first three topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, retrieve results for all the topics (may take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_final = BatchRetrieve(indexer_tunedParameters, wmodel=\"BM25\", controls=configuration4)\n",
    "run = bm25_final(pt_dataset.get_topics('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the retrieval. Here are the first 10 entries of the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Persist and upload run to TIRA\n",
    "\n",
    "The output of our retrieval system is a run file. This run file can later (and, e.g., in a different notebook or by a different person) be statistically evaluated. We will therefore first upload the run to TIRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import persist_and_normalize_run\n",
    "\n",
    "persist_and_normalize_run(\n",
    "    run,\n",
    "    # Give your approach a short but descriptive name tag.\n",
    "    system_name='bm25-modifiedStemmerWithTuning', \n",
    "    default_output='../data/runs',\n",
    "    upload_to_tira=pt_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier import Experiment \n",
    "\n",
    "Experiment(\n",
    "    [bm25_final],\n",
    "    topics = pt_dataset.get_topics(\"text\"),\n",
    "    qrels = pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut_10\"],\n",
    "    names=[\"BM25 with Tuned Parameters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link in the cell output above to claim your submission on TIRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Improve\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset.get_qrels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
